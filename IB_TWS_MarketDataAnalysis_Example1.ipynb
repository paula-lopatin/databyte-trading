{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfe29e-81fe-41e3-a2c6-a6be8f37e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def concatenate_excel_csv_files_no_glob(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all Excel (.xlsx, .xls) and CSV (.csv) files from a specified folder \n",
    "    without using glob, and concatenates them into a single pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the directory containing the files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single DataFrame containing all the data.\n",
    "    \"\"\"\n",
    "    # List to store individual DataFrames\n",
    "    list_dfs = []\n",
    "\n",
    "    # Iterate over all items in the directory\n",
    "\n",
    "    length = len(os.listdir(folder_path))\n",
    "    print(f\"len is {length}\")\n",
    "    \n",
    "    counter = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        counter = counter + 1\n",
    "        if counter % 100 == 0:\n",
    "            print(f\"{counter} /{length} {counter/length}\") \n",
    "        # Construct full file path\n",
    "        full_filepath = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Ensure it's a file and not a directory\n",
    "        if os.path.isfile(full_filepath):\n",
    "            if filename.endswith('.csv'):\n",
    "                df = pd.read_csv(full_filepath)\n",
    "                df['source_file'] = filename # Optional: add source file name\n",
    "                \n",
    "                # extracting out the symbol from the filenames\n",
    "                c = \"_\" # character\n",
    "                i= filename.index(c)\n",
    "                res = filename[:i]\n",
    "                df['symbol'] = res\n",
    "                \n",
    "                list_dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    if list_dfs:\n",
    "        # ignore_index=True ensures a continuous index for the combined DataFrame\n",
    "        combined_df = pd.concat(list_dfs, ignore_index=True) \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No files found to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "folder_directory = '.' \n",
    "merged_data = concatenate_excel_csv_files_no_glob(folder_directory)\n",
    "\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b9fd0-ce5e-48f3-ac01-661dc6b9fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33921611-ae6c-452a-a612-29e74a248d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f4987-69d4-4319-987c-05b3e3c73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['timestamp_dt'] = pd.to_datetime(merged_data['timestamp'])\n",
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed5443-8d66-4361-9782-66a888c5a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_data.copy()\n",
    "\n",
    "df['year'] = df['timestamp_dt'].dt.year\n",
    "df['month'] = df['timestamp_dt'].dt.month\n",
    "df['day'] = df['timestamp_dt'].dt.day\n",
    "df['hour'] = df['timestamp_dt'].dt.hour\n",
    "df['minute'] = df['timestamp_dt'].dt.minute\n",
    "df['second'] = df['timestamp_dt'].dt.second\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ca720-8410-4361-8404-8e2558a8d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c0224-4dea-4cd6-ac34-c74282377beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"TSLA\"\n",
    "tick_type = \"LAST\"\n",
    "\n",
    "df_subset = df.loc[(df[\"symbol\"]==symbol) & (df[\"tick_type\"] == tick_type)].copy()\n",
    "display(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11476c0c-799e-4aee-9245-91575dc8cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_subset.reset_index()\n",
    "\n",
    "df_subset[\"counter\"] = 1\n",
    "df_subset[\"counter\"] = df_subset[\"counter\"].cumsum()\n",
    "df_subset[\"hour_fraction\"] = df_subset[\"hour\"] + df_subset[\"minute\"]/60 + df_subset[\"second\"]/3600 + df_subset[\"counter\"]/3600/60\n",
    "display(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f91b49-16ec-4304-8e32-2d8a2d3b4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def aggregate_hourly_data(df, value_column):\n",
    "    hourly_stats = df.groupby('agg_col')[value_column].agg([\n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std'),\n",
    "        ('count', 'count'),\n",
    "        ('max', 'max'),\n",
    "        ('min', 'min'), \n",
    "        ('last', 'last')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    hourly_stats.rename(columns={'agg_cols': 'agg_col'}, inplace=True)\n",
    "    \n",
    "    return hourly_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290024d2-5508-4d58-85c0-963bde75d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_date_string(df, col1='year', col2='month', col3='day', col4='hour', col5='minute', \n",
    "                       new_col_name='subset_category', separator='_'):\n",
    "    \n",
    "    # Create the concatenated column\n",
    "    df[new_col_name] = (df[col1].astype(str) + separator + \n",
    "                        df[col2].astype(str) + separator + \n",
    "                        df[col3].astype(str) + separator + \n",
    "                        df[col4].astype(str) + separator + \n",
    "                        df[col5].astype(str))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb36e43-9e66-473b-a150-aebb7a1c68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = create_date_string(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b7e3f-740a-4698-85e2-42d4fdd32489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['agg_col'] = df_subset['source_file'].str[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7b26a-dcc4-43e1-8866-92819eec38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457025f-f714-48bc-8c42-9b93ecfeaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_agg = aggregate_hourly_data(df_subset, 'price')\n",
    "display(df_subset_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabc77d-10f2-4b0d-854b-8e04f74b41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_agg.to_csv(\"TSLA_test_data_agg_2025_12_16.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
